
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Neural Encoding II &#8212; Introductory Computational Neuroscience</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/NeuroImage.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Neural Encoding I" href="NeuralEncodingI.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/NeuroImage.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introductory Computational Neuroscience</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Coding
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="VisualizingNeuralResponses.html">
   Visualizing Neural Responses
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="TuningCurves.html">
   Tuning Curves
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Decoding_LinearRegression.html">
   Decoding: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Decoding_LogisticRegression.html">
   Decoding: Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="SpikeTriggeredAverages.html">
   Spike Triggered Averages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NeuralEncodingI.html">
   Neural Encoding I
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Encoding II
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/ebatty/IntroCompNeuro/blob/main/Notes/NeuralEncodingII.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/Notes/NeuralEncodingII.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Neural Encoding II
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-1-fitting-lnps-to-data">
   Section 1: Fitting LNPs to data
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-1-best-estimated-firing-rate-from-raster-plot">
     Section 1.1: Best estimated firing rate from raster plot
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-2-finding-the-linear-filter-for-data">
     Section 1.2: Finding the linear filter for data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-3-white-noise-stimuli">
     Section 1.3: White noise stimuli
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-2-benefits-drawbacks-of-lnps">
   Section 2: Benefits/drawbacks of LNPs
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-1-benefits">
     Section 2.1: Benefits
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-2-drawbacks">
     Section 2.2: Drawbacks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-3-model-comparison-analysis">
   Section 3: Model comparison analysis
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-reading">
   Optional Reading
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural Encoding II</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Neural Encoding II
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-1-fitting-lnps-to-data">
   Section 1: Fitting LNPs to data
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-1-best-estimated-firing-rate-from-raster-plot">
     Section 1.1: Best estimated firing rate from raster plot
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-2-finding-the-linear-filter-for-data">
     Section 1.2: Finding the linear filter for data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-3-white-noise-stimuli">
     Section 1.3: White noise stimuli
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-2-benefits-drawbacks-of-lnps">
   Section 2: Benefits/drawbacks of LNPs
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-1-benefits">
     Section 2.1: Benefits
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-2-drawbacks">
     Section 2.2: Drawbacks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-3-model-comparison-analysis">
   Section 3: Model comparison analysis
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-reading">
   Optional Reading
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <p><a href="https://colab.research.google.com/github/ebatty/IntroCompNeuro/blob/main/Notes/NeuralEncodingII.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="neural-encoding-ii">
<h1>Neural Encoding II<a class="headerlink" href="#neural-encoding-ii" title="Permalink to this headline">#</a></h1>
<p>Learning objectives of notes: After reading these notes, students should be able to:</p>
<ul class="simple">
<li><p>Describe how to fit LNPs to data</p></li>
<li><p>Explain the benefits and drawbacks of LNPs as an encoding model</p></li>
<li><p>Explain model comparison analysis and how it allows us to gain understanding from LNPs</p></li>
</ul>
<p>Imports</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="section-1-fitting-lnps-to-data">
<h1>Section 1: Fitting LNPs to data<a class="headerlink" href="#section-1-fitting-lnps-to-data" title="Permalink to this headline">#</a></h1>
<p>First, let’s recap the equations of an LNP model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\lambda_t = f(x_t \cdot k) \\
y_t \sim
 Poiss(\lambda_t)\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_t\)</span> is the stimulus we want to use to predict the neural reponse at time bin <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(k\)</span> is the linear filter, <span class="math notranslate nohighlight">\(\lambda_t\)</span> is the firing rate of the neuron in spikes per second, <span class="math notranslate nohighlight">\(y_t\)</span> is the spike count at time bin <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(f\)</span> is some nonlinearity. This is our model architecture - how we are specifying the mapping from stimulus to neural spikes in general. We need to learn the parameters of the model - in this case the linear filter k - based on data from a neuron though. Each neuron would have a different linear filter. We also might want to learn the nonlinearity <span class="math notranslate nohighlight">\(f\)</span> sometimes, but for now we will assume we have chosen it to be an exponential function.</p>
<p>So, how do we learn the linear filter for a neuron? We want to find the linear filter that leads to the best predictions of the neural response. In other words, the predicted spikes should be as similar as possible to the real spikes. It’s hard to come up with a metric that relates predicted to real spikes well since the exact timing is from Poisson draws and assumed under our model not to matter. Instead, we will find the linear filter that maximizes the probability of the true spikes given the model and stimulus. In other words, we want to find the linear filter that predicts a firing rate over time, <span class="math notranslate nohighlight">\(\lambda\)</span>, that results in the max probability of the spikes given that firing rate.</p>
<section id="section-1-1-best-estimated-firing-rate-from-raster-plot">
<h2>Section 1.1: Best estimated firing rate from raster plot<a class="headerlink" href="#section-1-1-best-estimated-firing-rate-from-raster-plot" title="Permalink to this headline">#</a></h2>
<p>Let’s backtrack to the second lecture - remember when we were working with raster plots and PSTHs? We computed the PSTH, or firing rate in spikes per bin, for each time bin as the mean number of spikes in that time bin over trials. Is this the firing rate that maximizes the probability, or likelihood, of those spikes if we assume a Poisson distribution? Let’s check.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown </span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">18</span><span class="p">})</span>

<span class="n">all_sp_time_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">11.1</span><span class="p">,</span> <span class="mf">15.6</span><span class="p">,</span> <span class="mf">16.7</span><span class="p">,</span> <span class="mf">27.8</span><span class="p">]),</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.4</span><span class="p">,</span> <span class="mf">11.9</span><span class="p">,</span> <span class="mf">15.9</span><span class="p">,</span> <span class="mf">17.7</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">8.6</span><span class="p">,</span> <span class="mf">10.6</span><span class="p">,</span> <span class="mf">13.2</span><span class="p">,</span> <span class="mf">14.2</span><span class="p">,</span> <span class="mi">27</span><span class="p">]),</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">8.9</span><span class="p">,</span> <span class="mf">11.9</span><span class="p">,</span> <span class="mf">12.9</span><span class="p">,</span> <span class="mf">14.5</span><span class="p">,</span> <span class="mf">26.3</span><span class="p">]),</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.4</span><span class="p">,</span> <span class="mf">8.2</span><span class="p">,</span> <span class="mf">10.6</span><span class="p">,</span> <span class="mf">13.2</span><span class="p">,</span> <span class="mf">16.9</span><span class="p">,</span> <span class="mi">28</span><span class="p">])]</span>

<span class="c1"># Make raster plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Make raster of this data</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">eventplot</span><span class="p">(</span><span class="n">all_sp_time_list</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="c1"># Label x and y axes (with units!)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span> <span class="c1">#xlabel = &#39;Time (ms)&#39;, </span>
       <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;Trial Number&#39;</span><span class="p">,</span>
       <span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span>
       <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">.7</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">],</span>
       <span class="n">yticks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
       <span class="n">yticklabels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]);</span>

<span class="c1"># Plot bin vertical lines </span>
<span class="n">bin_width</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">bin_edge_times</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">30.1</span><span class="p">,</span> <span class="n">bin_width</span><span class="p">)</span>
<span class="k">for</span> <span class="n">bin_time</span> <span class="ow">in</span> <span class="n">bin_edge_times</span><span class="p">:</span>
  <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">bin_time</span><span class="p">,</span> <span class="n">bin_time</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">.7</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">],</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="c1"># Make PSTH</span>

<span class="n">binned_spikes</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">all_sp_time_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">bins</span> <span class="o">=</span> <span class="n">bin_edge_times</span><span class="p">)</span>

<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">.1</span><span class="p">)</span>
<span class="n">y_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">i_bin</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bin_edge_times</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">y_vals</span><span class="p">[(</span><span class="n">x_vals</span> <span class="o">&gt;</span> <span class="n">bin_edge_times</span><span class="p">[</span><span class="n">i_bin</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x_vals</span> <span class="o">&lt;=</span> <span class="n">bin_edge_times</span><span class="p">[</span><span class="n">i_bin</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="n">binned_spikes</span><span class="p">[</span><span class="n">i_bin</span><span class="p">]</span> <span class="o">/</span> <span class="mi">5</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span>  <span class="s1">&#39;k&#39;</span><span class="p">);</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;Time (ms)&#39;</span><span class="p">,</span> 
            <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;Firing rate (spikes/bin)&#39;</span><span class="p">);</span>
            
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/NeuralEncodingII_7_0.png" src="../_images/NeuralEncodingII_7_0.png" />
</div>
</div>
<p>Remember from the last lecture that our Poisson distribution is:</p>
<div class="math notranslate nohighlight">
\[p(y_{t, k} | \lambda_t) = \frac{\lambda_t^{y_{t, k}} e^{-\lambda_{t}}}{y_{t,k}!} \]</div>
<p>where</p>
<p><span class="math notranslate nohighlight">\(y_{t, k}\)</span>: spike count for bin <span class="math notranslate nohighlight">\(t\)</span> and trial <span class="math notranslate nohighlight">\(k\)</span></p>
<p><span class="math notranslate nohighlight">\(\lambda_t\)</span>: firing rate in terms of spikes per bin</p>
<p><span class="math notranslate nohighlight">\(p(y_{t, k} | \lambda_t)\)</span>: the probability of the spike count being equal to some number given the firing rate</p>
<p>Let’s just look at a single time bin <span class="math notranslate nohighlight">\(t\)</span>. We want to find the <span class="math notranslate nohighlight">\(\lambda_t\)</span> that maximizes the probability of seeing the spike counds <span class="math notranslate nohighlight">\(y_{t, k}\)</span> for k from 1 to the number of trials, <span class="math notranslate nohighlight">\(n\)</span>. The likelihood of seeing those spike counts given <span class="math notranslate nohighlight">\(\lambda_t\)</span> is <span class="math notranslate nohighlight">\(L(\lambda_t)\)</span>.</p>
<div class="math notranslate nohighlight">
\[L(\lambda_t) = p(y_{t,1:n} | \lambda_t) \]</div>
<p>We assume the spike count on each trial is independent so we can turn this into a product of the probabilities of the spike counts on each trial:</p>
<div class="math notranslate nohighlight">
\[L(\lambda_t) = \prod_{k=1}^{n}p(y_{t, k} | \lambda_t) \]</div>
<p>We actually want the log of the likelihood (for reasons I won’t go into here). The log of a times b equals the log of a plus the log of b, so we can turn the product into a sum of logs.</p>
<div class="math notranslate nohighlight">
\[log(L(\lambda_t)) = \sum_{k=1}^{n}log(p(y_{t, k} | \lambda_t)) \]</div>
<p>Let’s substitute in what <span class="math notranslate nohighlight">\(p(y_{t,k}|\lambda_t)\)</span> equals and do some simplifying:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9f8efd21-ad87-4656-ae76-475dff4c6bbb">
<span class="eqno">(20)<a class="headerlink" href="#equation-9f8efd21-ad87-4656-ae76-475dff4c6bbb" title="Permalink to this equation">#</a></span>\[\begin{align}
log(L(\lambda_t)) &amp;= \sum_{k=1}^{n}log(p(y_{t, k} | \lambda_t))\\
&amp;= \sum_{k=1}^{n}log(\frac{\lambda_t^{y_{t, k}} e^{-\lambda_{t}}}{y_{t,k}!}) \\
&amp;= \sum_{k=1}^{n} [log(\lambda_t^{y_{t,k}}) + log(e^{-\lambda_t}) - log(y_{t,k}!)]\\
&amp;= \sum_{k=1}^{n} [y_{t,k}log(\lambda_t) -\lambda_t - log(y_{t,k}!)]\\
\end{align}\]</div>
<p>We want to find the <span class="math notranslate nohighlight">\(\lambda_t\)</span> that maximizes this. The maximums and minimums of functions occur when the derivative is zero. We can compute the derivative of the log likelihood function with respect to <span class="math notranslate nohighlight">\(\lambda_t\)</span>, set it equal to 0, and solve for <span class="math notranslate nohighlight">\(\lambda_t\)</span>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-da811da7-5208-45f2-9869-c34a3874655e">
<span class="eqno">(21)<a class="headerlink" href="#equation-da811da7-5208-45f2-9869-c34a3874655e" title="Permalink to this equation">#</a></span>\[\begin{align} 
\frac{d log(L(\lambda_t))}{d \lambda_t} = \sum_{k=1}^n [ \frac{y_{k, t}}{\lambda_t} - 1] &amp;= 0 \\
\sum_{k=1}^n [ \frac{y_{k, t}}{\lambda_t}] - n &amp;= 0 \\
\sum_{k=1}^n [ \frac{y_{k, t}}{\lambda_t}] &amp;= n\\ 
 \frac{\sum_{k=1}^n y_{k, t}}{\lambda_t} &amp;= n\\
 \sum_{k=1}^n y_{k, t} &amp;= n\lambda_t \\
 \lambda_t &amp;= \frac{\sum_{k=1}^n y_{k, t}}{n}
\end{align}\]</div>
<p>The value of <span class="math notranslate nohighlight">\(\lambda_t\)</span> that maximizes the probability of seeing the spike counts under a Poisson distribution is the mean of those spike counts across trials, exactly what we were using to compute our PSTH!</p>
</section>
<section id="section-1-2-finding-the-linear-filter-for-data">
<h2>Section 1.2: Finding the linear filter for data<a class="headerlink" href="#section-1-2-finding-the-linear-filter-for-data" title="Permalink to this headline">#</a></h2>
<p>The process of finding the linear filter based on the data from a neuron is a similar idea. We want to maximize the log likelihood of the spikes given <span class="math notranslate nohighlight">\(\lambda\)</span>. Instead of looking over trials, we will now look over time bins, we want to maximize the probability of the spike counts in multiple time bins given the <span class="math notranslate nohighlight">\(\lambda_t\)</span> in each time bin.</p>
<div class="amsmath math notranslate nohighlight" id="equation-2285030a-be4b-4e37-91d2-c923a4fe716b">
<span class="eqno">(22)<a class="headerlink" href="#equation-2285030a-be4b-4e37-91d2-c923a4fe716b" title="Permalink to this equation">#</a></span>\[\begin{align}
log(L(\lambda)) &amp;= \sum_{t=1}^{T} [y_{t}log(\lambda_t) -\lambda_t - log(y_{t}!)]\\
\end{align}\]</div>
<p>Now however, <span class="math notranslate nohighlight">\(\lambda_t\)</span> depends on our linear filter <span class="math notranslate nohighlight">\(k\)</span> and the stimulus.</p>
<div class="amsmath math notranslate nohighlight" id="equation-7413e329-fda9-4359-9bdf-557bbce46578">
<span class="eqno">(23)<a class="headerlink" href="#equation-7413e329-fda9-4359-9bdf-557bbce46578" title="Permalink to this equation">#</a></span>\[\begin{align}
\lambda_t &amp;= e^{x_t \cdot k}\\
log(L(\lambda)) &amp;= \sum_{t=1}^{T} [y_{t}log(\lambda_t) -\lambda_t - log(y_{t}!))]\\
\end{align}\]</div>
<p>We can not set the derivative of this equal to 0 and solve for <span class="math notranslate nohighlight">\(k\)</span>, it is impossible to distangle <span class="math notranslate nohighlight">\(k\)</span> from the dot product with <span class="math notranslate nohighlight">\(x_t\)</span>.  Instead we have to use other methods to find the <span class="math notranslate nohighlight">\(k\)</span> that maximizes this quantity. Most of these methods find parameters that minimize a function, not maximize, which we can easily adhere to by finding the filter <span class="math notranslate nohighlight">\(k\)</span> that minimizes the negative log likelihood (NLL):</p>
<div class="amsmath math notranslate nohighlight" id="equation-0b278a83-7554-4bf3-a5e3-f283218299b8">
<span class="eqno">(24)<a class="headerlink" href="#equation-0b278a83-7554-4bf3-a5e3-f283218299b8" title="Permalink to this equation">#</a></span>\[\begin{align}
NLL &amp;= -\sum_{t=1}^{T} [y_{t}log(\lambda_t) -\lambda_t - log(y_{t}!)]\\
\end{align}\]</div>
<p>We can get rid of the last term - we want to find the linear filter <span class="math notranslate nohighlight">\(k\)</span> that minimizes the negative log likelihood and the last term is not affected by the linear filter - it will be a constant.</p>
<div class="amsmath math notranslate nohighlight" id="equation-8f26a8fc-8f51-483b-a896-666f433183b6">
<span class="eqno">(25)<a class="headerlink" href="#equation-8f26a8fc-8f51-483b-a896-666f433183b6" title="Permalink to this equation">#</a></span>\[\begin{align}
NLL &amp;= -\sum_{t=1}^{T} [y_{t}log(\lambda_t) -\lambda_t ]\\
\end{align}\]</div>
<p>We can vectorize this computation for ease of coding: if <span class="math notranslate nohighlight">\(\lambda\)</span> is a vector of our predicted firing rates over time and <span class="math notranslate nohighlight">\(y\)</span> is a vector of our spike counts over time, we can rewrite the above as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7d042ca8-def1-4dd2-83f7-dd9c64c97067">
<span class="eqno">(26)<a class="headerlink" href="#equation-7d042ca8-def1-4dd2-83f7-dd9c64c97067" title="Permalink to this equation">#</a></span>\[\begin{align}
NLL &amp;= - y \cdot log(\lambda) + \sum_{t=1}^T\lambda_t\\
\end{align}\]</div>
<p>Now that we have it simplified, how do we actually find the linear filter that minimizes this NLL? Luckily, we have a python function that finds the parameters that minimize a metric for us: <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code>. If you pass a function that computes the negative log likelihood given an input of a linear filter, it will find the minimizing linear filter.</p>
</section>
<section id="section-1-3-white-noise-stimuli">
<h2>Section 1.3: White noise stimuli<a class="headerlink" href="#section-1-3-white-noise-stimuli" title="Permalink to this headline">#</a></h2>
<p>If your stimuli is a white noise stimuli, the STA is a pretty good estimate for <span class="math notranslate nohighlight">\(k\)</span>. You can then find the nonlinearity as described in “Characterizing the Nonlinearity” in https://jov.arvojournals.org/article.aspx?articleid=2192881?</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="section-2-benefits-drawbacks-of-lnps">
<h1>Section 2: Benefits/drawbacks of LNPs<a class="headerlink" href="#section-2-benefits-drawbacks-of-lnps" title="Permalink to this headline">#</a></h1>
<section id="section-2-1-benefits">
<h2>Section 2.1: Benefits<a class="headerlink" href="#section-2-1-benefits" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Linear-nonlinear-Poisson models are a fairly simple encoding model and are fairly interpretable in terms of how they are capturing neural processing. The linear filter is similar to a receptive field.</p></li>
<li><p>These models are easy to fit. If we choose an exponential nonlinearity (or certain other ones), the negative log likelihood will be convex. This means that it is easier to learn the parameters, as we will learn about next lecture.</p></li>
<li><p>These models also have worked pretty well. We can get good predictions of neural responses for neurons in early sensory and motor areas.</p></li>
<li><p>These models are easy to add extensions to. People have altered these models to incorporate the spiking history of the neuron (and thus capture things like refractory period or spike frequency adaptation) or the responses of neighboring neurons. We can add more filters from the stimulus that interact in complex ways.</p></li>
</ul>
</section>
<section id="section-2-2-drawbacks">
<h2>Section 2.2: Drawbacks<a class="headerlink" href="#section-2-2-drawbacks" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>This model makes a lot of assumptions about the neural processing, including that the underlying firing rate on each trial is the same. Extensions mentioned in the previous section can alter this and incorporate spiking history.</p></li>
<li><p>The model is quite simple and often doesn’t capture more complex neural responses well.</p></li>
<li><p>The linear filters are a bit dangerous to overly interpret. The exact look can depend on various model fitting choices. Instead, it is more safe to use model comparison analysis to make conclusions from LNPs, as detailed in the next section.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="section-3-model-comparison-analysis">
<h1>Section 3: Model comparison analysis<a class="headerlink" href="#section-3-model-comparison-analysis" title="Permalink to this headline">#</a></h1>
<p>How do we make conclusions from linear-nonlinear-Poisson models? One way is to do <strong>model comparison analysis</strong>: we fit models with different inputs or structures and compare which best predict the neuron.</p>
<p>Let’s say we’re trying to figure out if a neuron is responding to an auditory stimulus or a visual stimulus. We can fit an LNP model with the auditory stimulus as input, a model with the visual stimulus as input, and one with both auditory and visual stimulus as input. We then evalute the model performance on <strong>held-out data</strong>, or data we have not used to learn the best linear filter. We will see next lecture why this is important.</p>
<p>If the model using the visual stimulus predicts neural responses much better than the auditory stimulus, we can infer that the neuron is primarily responding to the visual stimulus, at least within the context of this specific model.</p>
<p>In class, we talked about the model comparison analysis approach done in this paper: https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/s41593-019-0502-4. In this paper, they fit encoding models with various inputs related to the task the mouse was performing and movements of the mouse (moving its whiskers, legs, etc). They found that movements of the animal were important for predicting neural responses across the brain, even outside of motor cortices!</p>
<p>We can also change the structure of the model. Researchers added the trial-specific spiking responses of neighboring neurons to help predict the spiking responses of a neuron in this study: https://www.nature.com/articles/nature07140 . This improved prediction, so they concluded that the neurons were correlated beyond their dependence on the stimulus.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h1>
<p>LNPs are easily fit to data by finding the linear filter that minimizes the negative log likelihood of the spikes given the predicted firing rate. They are a simple descriptive encoding model that can be used to generate insights into what the neurons are responding to, and how.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="optional-reading">
<h1>Optional Reading<a class="headerlink" href="#optional-reading" title="Permalink to this headline">#</a></h1>
<p>Content:</p>
<p>Notes on Generalized Linear Models (LNPs): https://arxiv.org/pdf/1404.1999.pdf (Links to an external site.)</p>
<p>More in-depth look at fitting models: https://youtube.com/watch?v=m1w7oywzwpA (Links to an external site.)</p>
<p>Could also look at tutorials 1 and 2 for more practice: https://compneuro.neuromatch.io/tutorials/W1D4_GeneralizedLinearModels/student/W1D4_Tutorial1.html# (Links to an external site.)</p>
<p>Video diving further into these types of models:  https://youtube.com/watch?v=NXVG9ORBYXQ (Links to an external site.)</p>
<p>Related research papers:</p>
<p>Well-known paper in Nature 2008 using LNPs with spike history: https://www.nature.com/articles/nature07140 (Links to an external site.)</p>
<p>Paper using linear encoding models &amp; model comparison analysis: https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/s41593-019-0502-4Links to an external site.</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ebatty/IntroCompNeuro",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="NeuralEncodingI.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Neural Encoding I</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ella Batty<br/>
  
      &copy; Copyright 2021.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>