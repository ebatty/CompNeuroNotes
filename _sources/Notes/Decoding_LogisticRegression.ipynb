{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ebatty/IntroCompNeuro/blob/main/Notes/Decoding_LogisticRegression.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAWscPgJ_stT"
   },
   "source": [
    "# Decoding: Logistic Regression\n",
    "**Learning objectives of notes**:\n",
    "After these notes, students should be able to:\n",
    "- Describe difference between regression and classification\n",
    "- Describe math behind logistic regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_brAvWj4CDl8"
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 1: Regression vs classification\n",
    "\n",
    "In the last section, we talked about trying to decode speed as a weighted sum of the neural activity, aka by using linear regression. Speed is a continuous-valued variable - it can take on any number. What if we were trying instead to decode the decision of a mouse, whether it turned left or right? \n",
    "\n",
    " We can encode a left turn as 0 and a right turn as 1. However, linear regression is no longer a great choice of model here. We have two discrete choices, 0 vs 1, to predict so we should incorporate that into the model. Indeed, this is an example of classification.\n",
    "\n",
    "If we are trying to predict a continuously valued output, such as speed, we use **regression models**. If we are trying to predict the output as one of a set of classes or categories (such as left vs right), we use **classification models**.\n",
    "\n",
    "In these notes, we'll outline one classification model, which is extremely confusingly named **logistic regression**. It is not a regression model, so this name is very silly."
   ],
   "metadata": {
    "id": "YVQqEFR7OjOM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 2: Logistic Regression\n",
    "\n",
    "We will outline logistic regression by going through the steps of model fitting we outlined in the previous notes."
   ],
   "metadata": {
    "id": "ElUmNJkzOnKl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Define a mapping (model) from input to output\n",
    "\n",
    "Let's say we have the activities of 3 neurons and we're trying to decode the decision of the mouse (left turn vs right turn):\n",
    "\n",
    "\\begin{align}\n",
    "y&: \\text{decision, 0 if left turn, 1 if right}\\\\\n",
    "n_1, n_2, n_3 &: \\text{firing rates of neuron 1, neuron 2, neuron 3}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We actually are going to use a model quite similar to linear regression but with an extra cherry on top that accounts for this being a classification problem instead of a regression problem.\n",
    "\n",
    "The first step of our model will be identical to linear regression: we will compute an intermediate value z as the weighted sum of neural activity:\n",
    "\n",
    "\\begin{align}\n",
    "z &= w_0 + w_1n_1 + w_2n_2 + w_3n_3 = \\vec{w}\\cdot\\vec{n}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Next, we'll put z through a \"squishing function\". We want to get to the probability that the mouse turned left vs right. Probabilities have to be between 0 and 1, so we'll put through z through the sigmoid function, which ensures that the output is between 0 and 1.\n",
    "\n",
    "\\begin{align}\n",
    "z &= w_0 + w_1n_1 + w_2n_2 + w_3n_3\\\\\n",
    "p &= \\frac{1}{1+exp(-z)}\n",
    "\\end{align}\n",
    "\n",
    "Here is a plot of the sigmoid function to help visualize this squishing:\n",
    "\n"
   ],
   "metadata": {
    "id": "gvj_mVUwP-HS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "z = np.arange(-5, 5)\n",
    "p = 1/(1+np.exp(-z))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(z, p)\n",
    "ax.set(xlabel='z',\n",
    "       ylabel='p')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "jlf4nVOtRG10",
    "outputId": "8fe1a6ca-f4b1-4fba-9b3e-1e410e26d610"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'p'), Text(0.5, 0, 'z')]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have $p$, which is the probability that the mouse choise to turn right, the probability that y=1.\n",
    "\n",
    "We can predict which decision the mouse made based on $p$. If $p>=.5$, we predict that the mouse turned right. If $p<.5$, we predict that the mouse turned left.\n",
    "\n",
    "We now have a full model defined that goes from the inputs to a prediction for the output (decision):\n",
    "\n",
    "\\begin{align}\n",
    "z &= w_0 + w_1n_1 + w_2n_2 + w_3n_3\\\\\n",
    "p &= \\frac{1}{1+exp(-z)}\\\\\n",
    "\\hat{y} &=       \\begin{cases}\n",
    "      0 & \\text{if $p<0.5$}\\\\\n",
    "      1 & \\text{if $p>=0.5$}\\\\\n",
    "    \\end{cases} \n",
    "\\end{align}\n"
   ],
   "metadata": {
    "id": "kU5sRpmRRZ6a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Collect data: measurements of input/output pairs\n",
    "\n",
    "We record the mouse's decision on many trials, and the neural activity at the time of the decision. Each trial is a data point (input/output pair)"
   ],
   "metadata": {
    "id": "aOEd1O_XSE96"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Decide on loss function\n",
    "\n",
    "We now have our model and the data we want to use to fit it. We need a quantitative measure of how badly our model is doing, our **cost function**, or **loss function**.\n",
    "\n",
    "Given $p$, the probability that y=1 is p. The only other option for y is $y=0$, so we know the probability of $y=0$ has to be $1-p$ (since probabilities of all options have to add up to 1). We can rewrite this handily as:\n",
    "\n",
    "$$Prob(y) = p^y(1-p)^{1-y} $$\n",
    "\n",
    "where \n",
    "\\begin{align}\n",
    "z &= w_0 + w_1n_1 + w_2n_2 + w_3n_3\\\\\n",
    "p &= \\frac{1}{1+exp(-z)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Think through what the above equation equals for $y=0$ and $y=1$ - you'll see that it's equivalent to what I wrote out above! \n",
    "\n",
    "This is the binomial distribution, it's most known for determining the probability of seeing heads or tails on a coin flip,. In that scenario, p tells us the bias of a coin. It would be 0.5 for most fair coins (equal likelihoods of seeing heads vs tails) but could be different for a biased coin used by a magician for example.\n",
    "\n",
    "We now have an equation for the probability of seeing our data given our model (since p is given by the model). We want to find the values for the weights that **maximize** the probability of seeing our data! \n",
    "\n",
    "First though, we need to change this so that we are accounting for more than one data point. Remember we have data from multiple trials. We want to fit our model to maximize the probability of seeing our data given our model *across all trials*. Remember that you can multiply together probabilities of independent events. So for example, to get the probability of seeing 3 heads in a row on a toin coss, you would multiple together the probability of seeing a head 3 times. p(HHH) = p(H)*p(H)*p(H) = 0.5 * 0.5 * 0.5.\n",
    "\n",
    "Using the same logic, the probability of seeing all our data across trials equals the probability of seeing the data on each trials multiplied together. Let's say we have trials i=1 to i=N. The decision on trial i is $y_i$ and the neural activity of neuron 1 on trial i is $n_{1, i}$, of neuron 2 is $n_{2, i}$ and so on.\n",
    "\n",
    "\\begin{align}\n",
    "Prob(y_1, y_2, ..., y_N) &= \\prod_{i=1}^N Prob(y_i)\\\\&= \\prod_{i=1}^N p_i^{y_i}(1-p_i)^{1-y_i} \n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\\begin{align}\n",
    "z_i &= w_0 + w_1n_{1, i} + w_2n_{2, i} + w_3n_{3, i}\\\\\n",
    "p_i &= \\frac{1}{1+exp(-z_i)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We want to find the values of the weights that maximizes this overall probability to fit our model. Since we usually try to minimize things, we can just make this negative: we want to minimize the negative probability of seeing our data given the model"
   ],
   "metadata": {
    "id": "eXoj9lxlSMj-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Learn model parameters from data\n",
    "\n",
    "We won't dive into exactly how we find these weights, we have handy code that does it for us. `sklearn` has a `LogisticRegression` class that will fit our model for us (you'll explore this in homework 1)."
   ],
   "metadata": {
    "id": "z9ilouvfVHhF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Evaluate performance on held-out data\n",
    "\n",
    "Once we have fit our model, we can evaluate our model on held-out data. We can report the accuracy of our model: the percentage of the time that we predicted the right category. \n",
    "\n",
    "**Note we didn't explicitly fit our model to maximize accuracy, because this doesn't work well for reasons we don't go into. However, it's a nice interpretable measure of how our model performs that we can still report!**"
   ],
   "metadata": {
    "id": "bF0-BamTVXS7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{admonition} **Stop and think!** Let's say the true decisions on 6 trials were $y=[0, 1, 1, 0, 0, 1]$. Using your learned logistic regression model, your predictions are $\\hat{y} = [1, 0, 1, 0, 0, 1]$. What is the accuracy of your model?\n",
    ":class: tip, dropdown\n",
    "We were incorrect in our predictions on two of the trials (the first two) and correct on four (the last four). So our accuracy is 4/6, or 66%."
   ],
   "metadata": {
    "id": "RRrcyOa-VyPk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{admonition} **Stop and think!** What is the chance accuracy in the above example? Let's say the training data contained 40% left turn decisions (y=0) and 60% right turn decisions (y=1).\n",
    ":class: tip, dropdown\n",
    "If the neurons didn't contain any information about decisions, we'd be best off just guessing the most common decision in the training data as our prediction for all test trials. In this case, we'd guess right turn decisions, which would make our accuracy on the test data 50%. So our baseline/chance test accuracy would be 50%"
   ],
   "metadata": {
    "id": "u5kbWCv5WXRf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3: Multinomial logistic regression\n",
    "\n",
    "Instead of two categories (left vs right decision), let's say we're trying to predict which of 6 categories occured. For example, the mouse is now choosing 1 of 6 directions to go in (hallways branching off of the central point where the mouse is located). We want to predict which hallway they went down from neural activity.\n",
    "\n",
    "\n",
    "\n",
    "We'll have weights from the neurons to predict the probability of each class (hallway) separately. \n",
    "\n",
    "We can drop the sigmoid function and replace with an exponential function. The reason we can do this is that we want to constrain the outputs to be positive, but we don't need to constain to between 0 and 1 using the sigmoid. Instead, we'll normalize all the exponentiated dot products over all the hallways to compute the probability of each hallway (so they all sum to 1):\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "z_h &= \\vec{w}_h\\cdot\\vec{n}_i\\\\\n",
    "\\end{align}\n",
    "\n",
    "Compute the above for all 6 values of h (the six hallways). Then normalize by the sum so all 6 probabilities sum to 1:\n",
    "\n",
    "\\begin{align}\n",
    "Prob(h) &= \\frac{e^{z_h}}{\\sum_{j=1}^{6} e^{z_j} }\\\\\n",
    "\\end{align}\n",
    "\n",
    "This gives us 6 values: the probabilities that mouse chose each of the 6 hallways. Our prediction can be the hallway with the highest probability. As in the two class logistic regression case, we will learn the weights to maximize the probability of seeing our data given the model/weights (not derived here)\n"
   ],
   "metadata": {
    "id": "ed31f4GjXbA1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{admonition} **Stop and think!** Let's say we have two neurons and are decoding which hallway the mouse took of 3. The neural activity vector is [1, $n_1$, $n_2$]. You fit the multinomial model and find that $w_{h=1}$ = [1, -1, 5], $w_{h=2}$ = [0, 1, 4], and $w_{h=3}$ = [-2, -1, 0]. For a trial where neuron 1 has activity of 4 and neuron 2 has activity of 1, what hallway would you predict the mouse took?\n",
    ":class: tip, dropdown\n",
    "\n\\begin{align}\nz_{h=1} &= [1, -1, 5]\\cdot[1, 4, 1] = 2\\\\\nz_{h=2} &= [0, 1, 4]\\cdot[1, 4, 1] = 8\\\\\nz_{h=3} &= [-2, -1, 0]\\cdot[1, 4, 1] = -6\\\\\n\\end{align}\n\nNow we normalize:\n\n\\begin{align}\nProb(h=1) &= \\frac{e^{z_{h=1}}}{\\sum_{j=1}^{3} e^{z_{h=j}}} = \\frac{e^2}{e^2 + e^8 + e^{-6}} = 0.0025\\\\\nProb(h=2) &= \\frac{e^{z_{h=2}}}{\\sum_{j=1}^{3} e^{z_{h=j}}} = 0.9975\\\\\nProb(h=3) &= \\frac{e^{z_{h=-6}}}{\\sum_{j=1}^{3} e^{z_{h=j}}} = 0\\\\\n\\end{align}\n\nNote these three probabilities now add up to 0, as they should! It is clear that we would predict the mouse took the second hallway, since the probability of the mouse taking that hallway given the model and neural activity is extremely high."
   ],
   "metadata": {
    "id": "Ua8vDZhpj0Zx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{admonition} **Stop and think!** If you have 7 neurons and are decoding which hallway the mouse took out of 8 hallways, how many weights will you learn?\n",
    ":class: tip, dropdown\n",
    "You'll learn a weight from each neuron for each hallway: 7 neurons * 8 hallways = 56 weights."
   ],
   "metadata": {
    "id": "ZfzLZXJnjlE-"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Dkm_4ZfHkt2F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "6vbfijWJkW8T"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}