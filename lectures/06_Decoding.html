
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>06 - Decoding &#8212; Introductory Computational Neuroscience</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/NeuroImage.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="05 - Neural Encoding II" href="05_NeuralEncodingII.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/NeuroImage.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introductory Computational Neuroscience</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neural Coding
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="02_NeuralResponses%26TuningCurves.html">
   02 - Neural Responses &amp; Tuning Curves
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_SpikeTriggeredAverages.html">
   03 - Spike Triggered Averages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_NeuralEncodingI.html">
   04 - Neural Encoding I
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_NeuralEncodingII.html">
   05 - Neural Encoding II
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   06 - Decoding
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/lectures/06_Decoding.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/ebatty/IntroCompNeuro/blob/main/lectures/06_Decoding.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   06 - Decoding
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-1-intro-to-decoding">
   Section 1: Intro to decoding
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-1-examples-of-decoding">
     Section 1.1: Examples of decoding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-1-reconstructing-speech-from-auditory-cortex">
       Example 1: Reconstructing speech from auditory cortex
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-2-reconstructing-faces-from-fmri">
       Example 2: Reconstructing faces from fMRI
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-2-why-use-neural-decoding">
     Section 1.2: Why use neural decoding?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-2-model-fitting-with-linear-decoding">
   Section 2: Model fitting with linear decoding
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-1-define-a-mapping-model-from-input-to-output">
     Step 1: Define a mapping (model) from input to output
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-2-collect-data-measurements-of-input-output-pairs">
     Step 2: Collect data: measurements of input/output pairs
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-3-decide-on-loss-function">
     Step 3: Decide on loss function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-4-learn-model-parameters-from-data">
     Step 4: Learn model parameters from data
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#option-1-find-best-parameters-by-hand">
       Option 1: Find best parameters by hand
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#option-2-find-best-parameters-by-analytical-solution">
       Option 2: Find best parameters by analytical solution
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#option-3-find-best-parameters-by-gradient-descent">
       Option 3: Find best parameters by gradient descent
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#step-5-evaluate-performance-on-held-out-data">
     Step 5: Evaluate performance on held-out data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-fitting-recap-extension">
     Model fitting recap &amp; extension
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-3-final-thoughts-on-linear-decoding">
   Section 3: Final thoughts on linear decoding
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-readings">
   Optional readings
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/ebatty/IntroCompNeuro/blob/main/lectures/06_Decoding.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="decoding">
<h1>06 - Decoding<a class="headerlink" href="#decoding" title="Permalink to this headline">¶</a></h1>
<p><strong>Learning objectives of lecture/notes</strong>:
After lecture, students should be able to:</p>
<ul class="simple">
<li><p>Identify what conclusions you can make about neural populations from decoding stimuli or behavior well</p></li>
<li><p>Identify general steps of model fitting process</p></li>
<li><p>Explain linear decoding model: how the stimulus or behavior is modeled as a function of neural responses</p></li>
</ul>
<p>Some of the code/demos are from <a class="reference external" href="https://compneuro.neuromatch.io/tutorials/W1D3_ModelFitting/student/W1D3_Tutorial1.html">Neuromatch W1D3 Model Fitting</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">widgets</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-1-intro-to-decoding">
<h1>Section 1: Intro to decoding<a class="headerlink" href="#section-1-intro-to-decoding" title="Permalink to this headline">¶</a></h1>
<img alt="https://raw.githubusercontent.com/ebatty/IntroCompNeuro/main/images/NeuralCoding.jpg" src="https://raw.githubusercontent.com/ebatty/IntroCompNeuro/main/images/NeuralCoding.jpg" />
<p>We have been focusing so far on neural encoding: trying to predict the neural response given some stimulus or behavioral variable. We will now turn to the opposite problem. We now want to predict (also called <strong>decode</strong> or <strong>reconstruct</strong>) the stimulus or behavioral variable from the neural responses.</p>
<p>In neural decoding, we learn some mapping (a model) from the neural responses to the sensory input or behavioral variable from data. We can then use this mapping to predict the experienced sensory input or behavioral variable for new instances of neural responses. There are lots of options for what model to use to map from responses to stimulus/behavior, ranging from quite simple to very complex, deterministic to probabilistic. Later in these notes, we will learn about the most common type of decoding model: linear decoding.</p>
<p>For now, let’s see a few examples of cool decoding to get excited.</p>
<div class="section" id="section-1-1-examples-of-decoding">
<h2>Section 1.1: Examples of decoding<a class="headerlink" href="#section-1-1-examples-of-decoding" title="Permalink to this headline">¶</a></h2>
<div class="section" id="example-1-reconstructing-speech-from-auditory-cortex">
<h3>Example 1: Reconstructing speech from auditory cortex<a class="headerlink" href="#example-1-reconstructing-speech-from-auditory-cortex" title="Permalink to this headline">¶</a></h3>
<p>In 2019, Anumanchipalli et al published <a class="reference external" href="https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/s41586-019-1119-1">a paper</a> in Nature on decoding spoken sentences.</p>
<p><strong>Snapshot view:</strong> The researchers recorded neural activity in a human brain while they spoke sentences. They then trained a decoder to go from the neural activity to reconstruct the speech waveform. They got really good performance by creating a unique decoding model: instead of directly decoding the speech waveform, they decoded representations of articulatory movement (the movements of throat/mouth/jaw needed to speak). They then transformed these movements into speech acoustics.  Once they trained this decoding model, they found it was very accurate - participants listening to the decoded speech waveform could identify what the person had said - and even extended to cases where the person mimed speaking but didn’t actually produce sound.</p>
<p><strong>Why do we care:</strong> If we can decode intended speech from brain activity alone, we could build better speech neuroprosthetic technology to restore spoken communication.</p>
</div>
<div class="section" id="example-2-reconstructing-faces-from-fmri">
<h3>Example 2: Reconstructing faces from fMRI<a class="headerlink" href="#example-2-reconstructing-faces-from-fmri" title="Permalink to this headline">¶</a></h3>
<p>In 2019, VanRullen et al published <a class="reference external" href="https://www.nature.com/articles/s42003-019-0438-y">a paper</a> on reconstructing faces from fMRI data.</p>
<p><strong>Snapshot view:</strong> VanRullen et al presented several thousand faces to humans while using an fMRI to measure brain activity. They then learned a decoding model from the fMRI activation patterns to reconstruct the face presented. To test their decoder, they presented new images to the humans (that they hadn’t seen before) and decoded them from the neural activity alone. The decoded faces pretty closely resembled the original ones (see Figure 4a in paper - the left column is the presented face, the two right columns are the decoded faces using two different decoding models)!</p>
<p><strong>Why do we care:</strong> This paper was more of a methods paper showing a new decoding model type that performed well - it didn’t use this method to ask and answer scientific questions. Building well performing decoding models/methods is critical for using decoding to understand the brain though!</p>
</div>
</div>
<div class="section" id="section-1-2-why-use-neural-decoding">
<h2>Section 1.2: Why use neural decoding?<a class="headerlink" href="#section-1-2-why-use-neural-decoding" title="Permalink to this headline">¶</a></h2>
<img alt="https://raw.githubusercontent.com/ebatty/IntroCompNeuro/main/images/decoding.png" src="https://raw.githubusercontent.com/ebatty/IntroCompNeuro/main/images/decoding.png" />
<p><strong>What can you conclude if you can decode a stimulus or behavioral variable well from a certain group of neurons?</strong></p>
<p>If we can decode a stimulus or behavioral variable well from a population of neurons, we know that that information is present in the responses of that neural population. We can’t conclude that this neural population is important or critical for processing that information or affecting behavior though! We’d to do targeted experiments, like ablating the neural population, to make a stronger conclusion about that.</p>
<p>If we cannot decode the variable of interest, information about that variable may not be present in the neural population. Alternatively, our model could not be sophisticated/good enough to capture that the information is present if it is present in a complex format.</p>
<p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0959438818301004?via%3Dihub">This paper</a> is a great opinion piece on interpreting encoding and decoding models.</p>
<p><strong>So why use neural decoding if we can’t make very strong claims?</strong></p>
<ul class="simple">
<li><p>Neural decoding has a lot of applications! We use neural decoding in neural prosthetics and brain-computer interfaces (to try to predict what a person wants to do just based on their brain signals).</p></li>
<li><p>Even just knowing what information is represented where in the brain is useful. We’ll dive more into this in a couple classes.</p></li>
<li><p>You can use <strong>model comparison analysis</strong>. For example, you could decode the stimulus from just one type of neuron and compare to the quality when decoding from another cell type. This could tell you about whether these cell types are representing different information about the stimulus.</p></li>
</ul>
</div>
</div>
<div class="section" id="section-2-model-fitting-with-linear-decoding">
<h1>Section 2: Model fitting with linear decoding<a class="headerlink" href="#section-2-model-fitting-with-linear-decoding" title="Permalink to this headline">¶</a></h1>
<p>In this section, we will outline a common decoding model, linear decoding (also called linear regression more generally), and how to fit this model to data. We’ll review the general process of **model fitting **(learning models from data).</p>
<p>We will use an example where we are trying to decode running speed of a mouse (<span class="math notranslate nohighlight">\(s\)</span>)from the firing rates of 3 neurons (<span class="math notranslate nohighlight">\(n_1, n_2, n_3\)</span>). We are ignoring time, so we’re treating each time bin as independent and predicting the speed in the bin based only on the neural responses in the same bin.</p>
<div class="section" id="step-1-define-a-mapping-model-from-input-to-output">
<h2>Step 1: Define a mapping (model) from input to output<a class="headerlink" href="#step-1-define-a-mapping-model-from-input-to-output" title="Permalink to this headline">¶</a></h2>
<p>In order to fit a model to data, we first need to define our model. In other words, we need to define the mapping from input to output with some learnable parameters.</p>
<p>Here, we will choose a common decoding method: <strong>linear decoding/regression.</strong> In this model, every feature of the stimulus or behavior is decoded as a weighted sum of the neural activity (often plus a constant term).</p>
<div class="amsmath math notranslate nohighlight" id="equation-5bf12c34-e7d4-4f7c-9df1-3cd58b2ab0a9">
<span class="eqno">(11)<a class="headerlink" href="#equation-5bf12c34-e7d4-4f7c-9df1-3cd58b2ab0a9" title="Permalink to this equation">¶</a></span>\[\begin{align}
s&amp;: \text{value of the running speed}\\
n_1, n_2, n_3 &amp;: \text{firing rates of neuron 1, neuron 2, neuron 3}\\
s &amp;= w_0 + w_1n_1 + w_2n_2 + w_3n_3\\
\end{align}\]</div>
<p>The weights <span class="math notranslate nohighlight">\(w_0\)</span>, <span class="math notranslate nohighlight">\(w_1\)</span>, <span class="math notranslate nohighlight">\(w_2\)</span>, and <span class="math notranslate nohighlight">\(w_3\)</span> are what we need to learn from data. <span class="math notranslate nohighlight">\(w_0\)</span> is a constant term: for example in a 1d line, this would be the y-intercept.</p>
<p>We can write this model as a dot product:
$<span class="math notranslate nohighlight">\(s = \bar{w}\cdot \bar{n}\)</span><span class="math notranslate nohighlight">\( 
where \)</span>\bar{w}<span class="math notranslate nohighlight">\( is the vector of weights and \)</span>\bar{n}<span class="math notranslate nohighlight">\( is the vector of neural firing rates plus an additional term of 1 to account for the constant:
\)</span><span class="math notranslate nohighlight">\(\bar{w} = \begin{bmatrix}w_0 \\ w_1 \\ w_2 \\ w_3\end{bmatrix},  \bar{n} = \begin{bmatrix}1 \\ n_1 \\ n_2 \\ n_3\end{bmatrix} \)</span>$</p>
<p>We now have our model for how the input (neural responses) is transformed to the output (running speed) - we just need to learn our weight values.</p>
</div>
<div class="section" id="step-2-collect-data-measurements-of-input-output-pairs">
<h2>Step 2: Collect data: measurements of input/output pairs<a class="headerlink" href="#step-2-collect-data-measurements-of-input-output-pairs" title="Permalink to this headline">¶</a></h2>
<p>In order to learn the best weights to predict outputs, we need lots of examples of input and output pairs. By input and output pair in this context, I mean the neural responses in a time bin (inputs) and simutaneously recorded mouse running speed (output).  To get lots of data, we can record the neural responses and speed for 10000 time bins. We can use vectors/matrices to store this data:</p>
<div class="amsmath math notranslate nohighlight" id="equation-50393065-6f72-4dc1-b11d-5842126151b3">
<span class="eqno">(12)<a class="headerlink" href="#equation-50393065-6f72-4dc1-b11d-5842126151b3" title="Permalink to this equation">¶</a></span>\[\begin{align}
\bar{s} = \begin{bmatrix}s \text{ for time bin 1} \\
s \text{ for time bin 2} \\ ...\\ \end{bmatrix}
\end{align}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-1ebf42f9-37b1-4daf-90f1-4901ab5aea0f">
<span class="eqno">(13)<a class="headerlink" href="#equation-1ebf42f9-37b1-4daf-90f1-4901ab5aea0f" title="Permalink to this equation">¶</a></span>\[\begin{align}
N = \begin{bmatrix}n_1 \text{ for time bin 1} &amp; n_2 \text{ for time bin 1} &amp; n_3 \text{ for time bin 1} \\
n_1 \text{ for time bin 2} &amp; n_2 \text{ for time bin 2} &amp; n_3 \text{ for time bin 2} \\
... &amp; ... &amp; ...
 \end{bmatrix}
\end{align}\]</div>
<p>We can then rewrite our model to account for our multiple data points as:
$<span class="math notranslate nohighlight">\(\bar{s} = N\bar{w}\)</span>$</p>
<p>Work through the matrix-vector multiplication yourself to convince yourself that this is the same model for each time bin as we wrote earlier with the dot product!</p>
</div>
<div class="section" id="step-3-decide-on-loss-function">
<h2>Step 3: Decide on loss function<a class="headerlink" href="#step-3-decide-on-loss-function" title="Permalink to this headline">¶</a></h2>
<p>We now have our model and the data we want to use to fit it. We need a quantitative measure of how badly our model is doing though - this is called a <strong>cost function</strong>, or <strong>loss function</strong>.  For our linear decoding model, we will use a broadly popular loss function called <strong>mean-squared error</strong>. The mean-squared error is the sum over data points of the difference between true output and predicted output, squared. In equation form:</p>
<div class="amsmath math notranslate nohighlight" id="equation-df69e1a0-6bc6-4fcd-b484-bf6b7261b87e">
<span class="eqno">(14)<a class="headerlink" href="#equation-df69e1a0-6bc6-4fcd-b484-bf6b7261b87e" title="Permalink to this equation">¶</a></span>\[\begin{align}
\text{Mean squared error (MSE) }&amp;= \sum_{i = 1}^{10000} (s_{\text{time bin i}} - \hat{s}_{\text{time bin i}})^2\\
\end{align}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\hat{s}_{\text{time bin i}}\)</span>  is the predicted value of the speed at time bin i, so:
$<span class="math notranslate nohighlight">\(\hat{s}_{\text{time bin i}} = \bar{n}_{\text{time bin i}} \cdot \bar{w}\)</span>$</p>
</div>
<div class="section" id="step-4-learn-model-parameters-from-data">
<h2>Step 4: Learn model parameters from data<a class="headerlink" href="#step-4-learn-model-parameters-from-data" title="Permalink to this headline">¶</a></h2>
<p>Now that we have all the necessary components, it’s time to actually learn the model parameters from data. There are several different ways of doing this. To take a look at this, we’ll simplify our decoding problem even more and decode the running speed based only on the firing rate of one neuron. This is for visualization purposes. We will also drop the constant term, so $<span class="math notranslate nohighlight">\(s = w_1*n \)</span>$</p>
<p>Execute the cell below to simulate some data and plot it.</p>
<p>Simulate and plot data</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Simulate and plot data</span>
<span class="c1"># setting a fixed seed to our random number generator ensures we will always</span>
<span class="c1"># get the same psuedorandom number sequence</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>

<span class="c1"># Let&#39;s set some parameters</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">1.2</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">30</span>

<span class="c1"># Draw x and then calculate y</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>  <span class="c1"># sample from a uniform distribution over [0,10)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>  <span class="c1"># sample from a standard normal distribution</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c1"># Plot the results</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>  <span class="c1"># produces a scatter plot</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;neural firing rate (n)&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;running speed (s)&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/06_Decoding_20_0.png" src="../_images/06_Decoding_20_0.png" />
</div>
</div>
<p>In the above plot, each data point corresponds to one time bin. It is the neural firing rate and running speed pair for that time bin. In the next cell, I create a function that returns the MSE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">w1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute the mean squared error</span>

<span class="sd">  Args:</span>
<span class="sd">    n (ndarray): An array of shape (samples,) that contains the input values.</span>
<span class="sd">    s (ndarray): An array of shape (samples,) that contains the corresponding</span>
<span class="sd">      measurement values to the inputs.</span>
<span class="sd">    w1 (float): An estimate of the slope parameter</span>

<span class="sd">  Returns:</span>
<span class="sd">    float: The mean squared error of the data with the estimated parameter.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Compute the estimated s</span>
  <span class="n">s_hat</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">n</span> 

  <span class="c1"># Compute mean squared error</span>
  <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">s</span> <span class="o">-</span> <span class="n">s_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">mse</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="option-1-find-best-parameters-by-hand">
<h3>Option 1: Find best parameters by hand<a class="headerlink" href="#option-1-find-best-parameters-by-hand" title="Permalink to this headline">¶</a></h3>
<p>In our simple example, we want to find the value of <span class="math notranslate nohighlight">\(w_1\)</span> that minimizes our mean-squared error (MSE) of our data. We could just try a bunch of different values for <span class="math notranslate nohighlight">\(w_1\)</span>, compute the MSE, and choose the best value of <span class="math notranslate nohighlight">\(w_1\)</span>. Try this in the demo below.</p>
<p>Make sure you execute this cell to enable the widget!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown Make sure you execute this cell to enable the widget!</span>

<span class="k">def</span> <span class="nf">plot_observed_vs_predicted</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">s_hat</span><span class="p">,</span> <span class="n">w1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; Plot observed vs predicted data</span>

<span class="sd">  Args:</span>
<span class="sd">    n (ndarray): observed x values</span>
<span class="sd">    s (ndarray): observed y values</span>
<span class="sd">    s_hat (ndarray): predicted y values</span>
<span class="sd">    w1</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Observed&#39;</span><span class="p">)</span>  <span class="c1"># our data scatter plot</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">s_hat</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fit&#39;</span><span class="p">)</span>  <span class="c1"># our estimated model</span>
  <span class="c1"># plot residuals</span>
  <span class="n">smin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s_hat</span><span class="p">)</span>
  <span class="n">smax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s_hat</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">smin</span><span class="p">,</span> <span class="n">smax</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
      <span class="n">title</span><span class="o">=</span><span class="sa">fr</span><span class="s2">&quot;MSE = </span><span class="si">{</span><span class="n">mse</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;n&#39;</span><span class="p">,</span>
      <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;s&#39;</span>
  <span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">w1</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">2.0</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">plot_data_estimate</span><span class="p">(</span><span class="n">w1</span><span class="p">):</span>
  <span class="n">s_hat</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">n</span>
  <span class="n">plot_observed_vs_predicted</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">s_hat</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "7a66c5f58e0141f0900abd07ddf9fd24"}
</script></div>
</div>
<p>The red line in the demo above is the fitted model for the current value of <span class="math notranslate nohighlight">\(w_1\)</span>. What’s the best value of <span class="math notranslate nohighlight">\(w_1\)</span>?</p>
<p>Around 1.2 seems to minimize the MSE.</p>
<p>We were able to find this parameter based on the data by hand but this is not a sustainable endeavour. Imagine you were trying to find the weights from even just 3 neurons (let alone thousands) - it would be impossible because you’d have to try a crazy amount of combinations. Luckily, there is an easier way.</p>
</div>
<div class="section" id="option-2-find-best-parameters-by-analytical-solution">
<h3>Option 2: Find best parameters by analytical solution<a class="headerlink" href="#option-2-find-best-parameters-by-analytical-solution" title="Permalink to this headline">¶</a></h3>
<p>Sometimes, we can find a straight-forward equation for the parameters that minimize the cost function based on the model. It turns out, we get lucky and can do this for linear regression.</p>
<p>In order to find the analytical solution, we can take the derivative of the loss function with respect to the parameter. We then set this equal to 0, and solve for the parameter. The derivative of the loss function is only 0 at a maximum or minimum (because the tangent line is horizontal there), so this gives us the value of the parameter that results in the maximum or minimum of the function. We should then prove it’s a minimum, in which case our equation gives us the parameter values that minimize the loss function.</p>
<p>I will not go through the derivation but the video below covers it starting at 6 minutes and 50 seconds. The whole video goes through linear regression so may be helpful!</p>
<p>If we do this derivation, we find that the best weight vector is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6db5a132-3ee6-4b39-93c2-bb22502bb0da">
<span class="eqno">(15)<a class="headerlink" href="#equation-6db5a132-3ee6-4b39-93c2-bb22502bb0da" title="Permalink to this equation">¶</a></span>\[\begin{align}
\bar{w} = (N^TN)^{-1}N^T\bar{s}
\end{align}\]</div>
<p>This means that we can very easily fit the model by inputting our data into this equation and finding our estimate of <span class="math notranslate nohighlight">\(\bar{w}\)</span>! This is called the <strong>ordinary least squares solution</strong>.</p>
<p>Video: Linear Regression</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Video available at https://youtube.com/watch?v=DmoTblsbsCo
</pre></div>
</div>
<div class="output text_html">
<iframe
    width="730"
    height="410"
    src="https://www.youtube.com/embed/DmoTblsbsCo?fs=1"
    frameborder="0"
    allowfullscreen

></iframe>
</div></div>
</div>
<p>Let’s use this equation to estimate <span class="math notranslate nohighlight">\(w_1\)</span> on our data. Note that it looks a little different in the code because we only have a single parmeter, which changes how some of the vector notation works.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w1</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">n</span><span class="p">)</span><span class="o">**-</span><span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">s</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.2084421298265953
</pre></div>
</div>
</div>
</div>
<p>This is pretty close to what we guessed <span class="math notranslate nohighlight">\(w_1\)</span> was when we were doing it by hand in Option 1!</p>
</div>
<div class="section" id="option-3-find-best-parameters-by-gradient-descent">
<h3>Option 3: Find best parameters by gradient descent<a class="headerlink" href="#option-3-find-best-parameters-by-gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>Sometimes, we don’t get so lucky and we don’t have a nice equation for the parameters that minimize our loss function. In this case, we’ll have to find these parameters using alternative methods. One very common method that we’ll focus on is called gradient descent.</p>
<p>The gif below (from Neuromatch W2D1) shows the basic idea of gradient descent.</p>
<p>Execute this cell to view gradient descent gif</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown Execute this cell to view gradient descent gif</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s1">&#39;https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/static/grad_descent.gif?raw=true&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><img src="https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/static/grad_descent.gif?raw=true"/></div></div>
</div>
<p>So what’s happening here?</p>
<p>We are looking at a case where we just have a single parameter <span class="math notranslate nohighlight">\(w\)</span> for now. This is plotted on the x-axis. The loss function <span class="math notranslate nohighlight">\(L(w)\)</span> is on the y-axis.</p>
<p>We start our gradient descent algorithm by choosing a random value for <span class="math notranslate nohighlight">\(w\)</span>: <span class="math notranslate nohighlight">\(w^0\)</span>. We then change this value based on the derivative of the loss function at that value (the gradient). We want to move down further into the valley so we want to step in the negative direction of the slope (so we’re heading downhill). We compute this as:
$<span class="math notranslate nohighlight">\(w^1 = w^0 - \alpha \frac{dL}{dw^0}\)</span>$</p>
<p>Our new guess for the parameter (<span class="math notranslate nohighlight">\(w^1\)</span>) is the old guess (<span class="math notranslate nohighlight">\(w^0\)</span>) minus some value <span class="math notranslate nohighlight">\(\alpha\)</span> times the derivative of the loss function at that point. We keep doing this iteratively so:
$<span class="math notranslate nohighlight">\(w^{i+1} = w^{i} - \alpha \frac{dL}{dw^{i}}\)</span>$</p>
<p>If we keep taking small steps, we will eventually get to a minimum, where the derivative will equal 0 so the estimate of <span class="math notranslate nohighlight">\(w\)</span> will no longer change, and we’ll know to stop. If our loss function is <strong>convex</strong>, there is only one minimum and it is a global minimum. If we take small enough steps, we will always reach that global minimum and find the best parameters with gradient descent. This is why we want convex loss functions!</p>
<p>If our loss function is <strong>non-convex</strong>, there can be local minimums (valleys in the loss function that aren’t the absolute minimum value). Depending on the choice of your initial guess, gradient descent could get stuck in a local minimum: we would not find the parameter that leads to the actual minimum of the loss function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Difficult challenge: implement gradient descent for our example yourself! Otherwise, check</span>
<span class="c1"># back here for the implementation after people have a chance to try.</span>
</pre></div>
</div>
</div>
</div>
<p>We demonstrated gradient descent with just one parameter, but the idea extends to multiple parameters and a multi-dimensional loss landscape.</p>
</div>
</div>
<div class="section" id="step-5-evaluate-performance-on-held-out-data">
<h2>Step 5: Evaluate performance on held-out data<a class="headerlink" href="#step-5-evaluate-performance-on-held-out-data" title="Permalink to this headline">¶</a></h2>
<p>We need to do one last thing in our model fitting procedure. Sometimes, the lowest loss function doesn’t actually indicate the best model. Check out the data and model fits (dashed lines below). Image from: https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76</p>
<img alt="https://miro.medium.com/max/1400/1*_7OPgojau8hkiPUiHoGK_w.png" src="https://miro.medium.com/max/1400/1*_7OPgojau8hkiPUiHoGK_w.png" />
<p>The right-most model fit would lead the lowest MSE. However, it’s not a very good model for the data. It is doing something called <strong>overfitting</strong>: it’s learned the noise present in the data. This shows that we don’t want to just evaluate our model performance on the same data we’re using to train it. Instead, we want to evaluate the performance on some held-out data, called test data. These are data points that were not used to find the best parameters (whether by analytical solution or gradient descent).</p>
<p>If we had recorded 10000 time bins of the running speed and neural activity for example, we could use 8000 bins to fit the model (called our <strong>training data</strong>) and 2000 bins to evalute the model (called our <strong>test data</strong>).</p>
<p>The image below is from https://vitalflux.com/overfitting-underfitting-concepts-interview-questions/.</p>
<img alt="https://vitalflux.com/wp-content/uploads/2020/12/overfitting-and-underfitting-wrt-model-error-vs-complexity.png" src="https://vitalflux.com/wp-content/uploads/2020/12/overfitting-and-underfitting-wrt-model-error-vs-complexity.png" />
</div>
<div class="section" id="model-fitting-recap-extension">
<h2>Model fitting recap &amp; extension<a class="headerlink" href="#model-fitting-recap-extension" title="Permalink to this headline">¶</a></h2>
<p>We now have our complete step-by-step process for fitting our model and evaluting how well we’re doing! We need our model definition, data, loss function. Then, we learn the model parameters from data, using either an analytical solution or gradient descent, and evaluate how well we’re doing on new test data.</p>
<p>We use these steps for fitting all models, including the linear-nonlinear-Poisson encoding models we’ve discussed previously. There, the loss function was the negative-log-likelihood of the spikes given a Poisson distribution and we have to learn the linear filter using gradient descent.</p>
<p>There’s lots more cool stuff to do with fitting models to data - it’s a huge field of study that includes machine learning and data science. Going further in-depth is outside the scope of the course but I definitely recommend taking a machine learning or data science course if this kind of thing interests you!</p>
</div>
</div>
<div class="section" id="section-3-final-thoughts-on-linear-decoding">
<h1>Section 3: Final thoughts on linear decoding<a class="headerlink" href="#section-3-final-thoughts-on-linear-decoding" title="Permalink to this headline">¶</a></h1>
<p>We’ve focused for a bit on procedure of fitting models to data so let’s return to our neuroscience focus: decoding. Linear decoding is just one option of a decoding model type, but it is extremely common. It’s a simple model that’s easy to understand and easy to fit. As with the linear-nonlinear-Poisson models, despite being quite simple, it seems to do a decent job at decoding stimuli and/or behavior in many cases.</p>
<p>Additionally, there is a scientific reason we might choose this type of model. Remember that we are looking at whether information is present in a neural population. That information would most likely by used (if it is used) by neurons upstream, meaning the neurons that this group projects to. Neurons are great at computing dot products and doing linear transformations of inputs. By using a linear decoding model, we are looking explicitly at whether the information is present in the neural responses in this linearly readable format, that upstream brain regions could figure out. For more explanation of this, see the opinion piece about interpreting neural encoding &amp; decoding models: https://www.sciencedirect.com/science/article/pii/S0959438818301004?via%3Dihub</p>
</div>
<div class="section" id="optional-readings">
<h1>Optional readings<a class="headerlink" href="#optional-readings" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Video on linear regression, with proof of analytical solution:
https://youtu.be/DmoTblsbsCo</p></li>
<li><p>Tutorial on linear regression &amp; model fitting: https://compneuro.neuromatch.io/tutorials/W1D3_ModelFitting/chapter_title.html</p></li>
<li><p>Tutorial on fitting encoding and decoding models: https://compneuro.neuromatch.io/tutorials/W1D4_GeneralizedLinearModels/chapter_title.html</p></li>
<li><p>Interesting opinion piece about interpreting neural encoding &amp; decoding models: https://www.sciencedirect.com/science/article/pii/S0959438818301004?via%3Dihub</p></li>
</ul>
</div>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"86b4417fa67a4ccdb003948504b4ecaa": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1fa17a8a50f743569624381a9310111c": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "7a74e1cd75934c4c9a01f3bc785a445a": {"model_name": "FloatSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "FloatSliderView", "continuous_update": true, "description": "w1", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_86b4417fa67a4ccdb003948504b4ecaa", "max": 2.0, "min": 0.0, "orientation": "horizontal", "readout": true, "readout_format": ".2f", "step": 0.1, "style": "IPY_MODEL_1fa17a8a50f743569624381a9310111c", "value": 1.0}}, "6e999196b8d046dbb1b4d023bacbc659": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7a66c5f58e0141f0900abd07ddf9fd24": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["widget-interact"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_7a74e1cd75934c4c9a01f3bc785a445a", "IPY_MODEL_18cb498df42a47ad9a01109746b41905"], "layout": "IPY_MODEL_6e999196b8d046dbb1b4d023bacbc659"}}, "8be4dfd9c2554ab8a5ac941cde9fdbd1": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "18cb498df42a47ad9a01109746b41905": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_8be4dfd9c2554ab8a5ac941cde9fdbd1", "msg_id": "", "outputs": [{"output_type": "display_data", "metadata": {"needs_background": "light"}, "data": {"text/plain": "<Figure size 432x288 with 1 Axes>", "image/png": "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\n"}}]}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ebatty/IntroCompNeuro",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="05_NeuralEncodingII.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">05 - Neural Encoding II</p>
            </div>
        </a>
    </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Ella Batty<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>